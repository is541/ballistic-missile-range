{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import GPy\n",
    "import mountain_car as mc\n",
    "import gym\n",
    "#from IPython.html.widgets import interact\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "np.random.seed(12345) # for reproducibility purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and optimization of a mountain car simulator\n",
    "\n",
    "*Javier Gonzalez* and *Mark Pullin*\n",
    "\n",
    "Amazon Cambridge, UK.\n",
    "\n",
    "\n",
    "## Index\n",
    "\n",
    "======================================================================================================================\n",
    "\n",
    "1. [Goal of this notebook](#1.-Goal-of-this-notebook)\n",
    "2. [The mountain car simulator](#2.-The-mountain-car-simulator)\n",
    "3. [Solving the mountain car example with an emulator of the reward](#3.-Solving-the-mountain-car-example-with-an emulator-of-the-reward)\n",
    "4. [Data efficient emulation of the car dynamics](#4.-Data-efficient-emulation-of-the-car-dynamics)\n",
    "5. [Conclusions](#5.-Conclusions)\n",
    "\n",
    "======================================================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Goal of this notebook\n",
    "\n",
    "In this notebook we use Emukit to illustrate different concepts of [Uncertainty Quantification](https://en.wikipedia.org/wiki/Uncertainty_quantification) (UQ) and the role that Gaussian processes play in this field. Based on a simple simulator of a car moving between a valley and a mountain, we are going to illustrate the following concepts:\n",
    "\n",
    "- ** Systems emulation**. Many real world decisions are based on simulations that can be computationally very demanding. We will show how simulators can be replaced by *emulators*: Gaussian process models fitted on a few simulations that can be used to replace the *simulator*. Emulators are cheap to compute, fast to run, and always provide ways to quantify the uncertainty of how precise they are compared the original simulator.\n",
    "\n",
    "\n",
    "- **Emulators in optimization problems**. We will show how emulators can be used to optimize black-box functions that are expensive to evaluate. This field is also called Bayesian Optimization and has gained an increasing relevance in machine learning as emulators can be used to optimize computer code (machine learning algorithms) quite efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The mountain car simulator\n",
    "\n",
    "\n",
    "To illustrate the above mentioned concepts we we use the [mountain car simulator](https://github.com/openai/gym/wiki/MountainCarContinuous-v0). This simulator is widely used in machine learning to test reinforcement learning algorithms. The goal is to define a control policy on a car whose objective is to climb a mountain. Graphically, the problem looks as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustratrion of one frame of the mountain car simulator\n",
    "env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
    "observation = env.reset()\n",
    "cum_reward = 0\n",
    "frame = []\n",
    "frame.append(env.render()) # mode = 'rgb_array'  # FIXED\n",
    "action = env.action_space.sample()\n",
    "observation, reward, done, truncated, info = env.step(action)\n",
    "#env.render(close=True) # FIXED\n",
    "env.close()\n",
    "\n",
    "# mc.display_frames_as_gif(frame,title='Mountain car') # NOT WORKING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The goal is to define a sequence of actions (push the car right or left with certain intensity) to make the car reach the flag after a number $T$ of time steps.\n",
    "\n",
    "At each time step $t$, the car is characterized by a vector $\\textbf{x}_{t} = (p_t,v_t)$ of states which are respectively the the position and velocity of the car at time $t$. For a sequence of states (an episode), the dynamics of the car is given by\n",
    "\n",
    "$$\\textbf{x}_{t+1} = f(\\textbf{x}_{t},\\textbf{u}_{t})$$\n",
    "\n",
    "where $\\textbf{u}_{t}$ is the value of an action force, which in this example corresponds to push car to the left (negative value) or to the right (positive value). The actions across a full episode are represented in a policy $\\textbf{u}_{t} = \\pi(\\textbf{x}_{t},\\theta)$ that acts according to the current state of the car and some parameters $\\theta$. In the following examples we will assume that the policy is linear which allows us to write $\\pi(\\textbf{x}_{t},\\theta)$ as\n",
    "\n",
    "$$\\pi(\\textbf{x},\\theta)= \\theta_0 + \\theta_p p + \\theta_vv. $$\n",
    "\n",
    "For $t=1,\\dots,T$ now given some initial state $\\textbf{x}_{0}$ and some some values of each $\\textbf{u}_{t}$, we can **simulate** the full dynamics of the car for a full episode using [Gym](https://gym.openai.com/envs/). The values of \n",
    "$\\textbf{u}_{t}$ are fully determined by the parameters of the linear controller.\n",
    "\n",
    "After each episode of length is complete, a reward function $R_{T}(\\theta)$ is computed. In the mountain car example the reward is computed as 100 for reaching the target of the hill on the right hand side, minus the squared sum of actions (a real negative to push to the left and a real positive to push to the right) from start to goal.  Note that our reward depend on $\\theta$ as we make it dependent on the parameters of the linear controller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solving the mountain car example with an emulator of the reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this section is to find the parameters $\\theta$ of the linear controller such that\n",
    "\n",
    "$$\\theta^* = arg \\max_{\\theta} R_T(\\theta).$$ \n",
    "\n",
    "In this section, we directly use Bayesian optimization to solve this problem. We will use Emukit so we fist define the objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_reward = lambda x: mc.run_simulation(env, x)[0]\n",
    "\n",
    "def reward_function(X):\n",
    "    rewards = np.empty(shape=[0, 1])\n",
    "    for i in range(X.shape[0]):\n",
    "        rewards = np.vstack([rewards, simulator_reward(X[i,None])])\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each set of parameter values of the linear controller we can run an episode of the simulator (that we fix to have a horizon of $T=500$) to generate the reward. Using as input the parameters of the controller and as outputs the rewards we can build a Gaussian process emulator of the reward. \n",
    "\n",
    "We start defining the input space, which is three-dimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core import ParameterSpace, ContinuousParameter\n",
    "parameter_space = ParameterSpace([ContinuousParameter('postion_parameter', -1.2, 1),\n",
    "                                  ContinuousParameter('velocity_parameter', -1/0.07, 1/0.07),\n",
    "                                  ContinuousParameter('constant', -1, 1),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initalize the model we start sampling some initial points (25) for the linear controler randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core.initial_designs import RandomDesign\n",
    "design = RandomDesign(parameter_space) \n",
    "X = design.get_samples(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start any optimization, lets have a look to the behavior of the car with the first of these initial points that we have selected randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m random_controller \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m----> 2\u001b[0m _, _, _, frames \u001b[38;5;241m=\u001b[39m \u001b[43mmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_controller\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m mc\u001b[38;5;241m.\u001b[39mdisplay_frames_as_gif(frames, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom linear controller\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/ballistic-missile-range/notebooks/Car_simulator_example/mountain_car.py:87\u001b[0m, in \u001b[0;36mrun_simulation\u001b[0;34m(env, controller_gains, render)\u001b[0m\n\u001b[1;32m     84\u001b[0m observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Initialize matrices to store state + control inputs\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m state_trajectory \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m0\u001b[39m, \u001b[43mobservation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     88\u001b[0m control_inputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray((\u001b[38;5;241m0\u001b[39m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     89\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "random_controller = X[0,:]\n",
    "_, _, _, frames = mc.run_simulation(env, np.atleast_2d(random_controller), render=True)\n",
    "mc.display_frames_as_gif(frames, 'Random linear controller')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the random linear controller does not manage to push the car to the top of the mountain. Now, let's optimize the regret using Bayesian optimization and the emulator for the reward. We try 50 new parameters chosen by the EI. We first create a GP model mapping controler parameters to rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPy.models import GPRegression\n",
    "from emukit.model_wrappers import GPyModelWrapper\n",
    "\n",
    "Y = reward_function(X)\n",
    "kernel = GPy.kern.Matern52(3)\n",
    "model_gpy = GPRegression(X,Y,kernel = kernel) \n",
    "model_gpy.Gaussian_noise.constrain_fixed(1e-8)  ## exact evaluations of the objective\n",
    "model_gpy.optimize_restarts(5)\n",
    "model_emukit = GPyModelWrapper(model_gpy)\n",
    "model_emukit.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian optimization an acquisition function is used to balance exploration and exploitation to evaluate new locations close to the optimum of the objective. In this notebook we select the expected improvement (EI). For further details have a look to the review paper of [Shahriari et al (2015)](http://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.bayesian_optimization.acquisitions import ExpectedImprovement\n",
    "\n",
    "expected_improvement = ExpectedImprovement(model = model_emukit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.bayesian_optimization.loops import BayesianOptimizationLoop\n",
    "\n",
    "bayesopt_loop = BayesianOptimizationLoop(model = model_emukit,\n",
    "                                         space = parameter_space,\n",
    "                                         acquisition = expected_improvement,\n",
    "                                         batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the loop for 50 extra evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core.loop import FixedIterationsStoppingCondition\n",
    "\n",
    "num_evaluations = 50\n",
    "stopping_condition = FixedIterationsStoppingCondition(i_max = num_evaluations)\n",
    "bayesopt_loop.run_loop(reward_function, stopping_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the result for the best controller that we have found with Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_opt = bayesopt_loop.loop_state.X[np.argmin(bayesopt_loop.loop_state.Y),:]\n",
    "_, _, _, frames = mc.run_simulation(env, np.atleast_2d(x_opt), render=True)\n",
    "mc.display_frames_as_gif(frames, 'Best controller after 50 iterations of Bayesian optimization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The car can now make it to the top of the mountain! Emulating the reward function and using the EI helped as to find a linear controller that solves the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data efficient emulation of the car dynamics\n",
    "\n",
    "In the previous section we solved the mountain car problem by directly emulating the reward but no considerations about the dynamics $\\textbf{x}_{t+1} = f(\\textbf{x}_{t},\\textbf{u}_{t})$ of the system were made. Note that we had to run 75 episodes of 500 steps each to solve the problem, which required to call the simulator $500\\times 75 =37500$ times. In this section we will show how it is possible to reduce this number by building an emulator for $f$ that can later be used to directly optimize the control.\n",
    "\n",
    "The inputs of the model for the dynamics are the velocity, the position and the value of the control so create this space accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core import ParameterSpace, ContinuousParameter\n",
    "\n",
    "parameter_space = ParameterSpace([ContinuousParameter('postion_parameter', -1.2, 1),\n",
    "                                  ContinuousParameter('velocity_parameter', -0.07, 0.07),\n",
    "                                  ContinuousParameter('constant', -1, 1),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are the velocity and the position. Indeed our model will capture the change in position and velocity on time. That is, we will model  \n",
    "\n",
    "$$\\Delta v_{t+1} = v_{t+1} - v_{t}$$\n",
    "$$\\Delta x_{t+1} = p_{t+1} - p_{t}$$\n",
    "\n",
    "with Gaussian processes with prior mean $v_{t}$ and $p_{t}$ respectively. As a covariance function, we use a Matern52.  We need therefore two models to capture the full dynamics of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sample some input parameters and use the simulator to compute the outputs. Note that in this case we are not running the full episodes, we are just using the simulator to compute $\\textbf{x}_{t+1}$ given $\\textbf{x}_{t}$ and $\\textbf{u}_{t}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core.initial_designs import RandomDesign\n",
    "\n",
    "# Random locations of the inputs\n",
    "n_initial_points = 500\n",
    "random_design_dynamics = RandomDesign(parameter_space) \n",
    "initial_design_dynamics = random_design_dynamics.get_samples(n_initial_points)\n",
    "\n",
    "# Simulation of the (normalized) outputs\n",
    "y = np.zeros((initial_design_dynamics.shape[0], 2))\n",
    "for i in range(initial_design_dynamics.shape[0]):\n",
    "    y[i, :] = mc.simulation(initial_design_dynamics[i, :])\n",
    "y_normalisation = np.std(y, axis=0)\n",
    "y_normalised = y/y_normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can be used smarter strategies to select the points. We could use the variance of the predictive distributions of the models to collect points using uncertainty sampling, which will give us a better coverage of the space. For simplicity, we move ahead with the 500 randomly selected points. \n",
    "\n",
    "Now that we have a data set, we can learn the emulators for the location and the velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPy.models import GPRegression\n",
    "\n",
    "kernel_mattern = GPy.kern.Matern52(3, variance = 1., ARD=True)\n",
    "noise_var_postition = y[:, [0]].var()*0.01\n",
    "noise_var_velocity = y[:, [1]].var()*0.01\n",
    "\n",
    "position_model = GPRegression(initial_design_dynamics, y[:, [0]], kernel = kernel_mattern.copy(), noise_var = noise_var_postition) \n",
    "velocity_model = GPRegression(initial_design_dynamics, y[:, [1]], kernel = kernel_mattern.copy(), noise_var = noise_var_velocity) \n",
    "\n",
    "position_model.Gaussian_noise.constrain_fixed(1e-6, warning=False)\n",
    "velocity_model.Gaussian_noise.constrain_fixed(1e-6, warning=False)\n",
    "\n",
    "position_model.optimize()\n",
    "velocity_model.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some input parameters of the linear controlling, how do the dynamics of the emulator and simulator match? In the following figure we show the position and velocity of the car for the 500 time steps of an episode in which the parameters of the linear controller have been fixed beforehand. The value of the input control is also shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller_gains = np.atleast_2d([0, .6, 1])  # change the valus of the linear controller to observe the trayectories.\n",
    "mc.plot_emu_sim_comparison(env, controller_gains, [position_model, velocity_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all! To complete this section let's make explicit use of the emulator. We now use it to replace the simulator and optimize the linear controller. Note that in this optimization, we don't need to query the simulator anymore as we can reproduce the full dynamics of an episode using the emulator. For illustrative purposes, in this example we fix the initial location of the car. \n",
    "\n",
    "We define the objective reward function in terms of the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize control parameters with emulator\n",
    "car_initial_location = np.asarray([-0.58912799, 0]) \n",
    "\n",
    "simulator_reward_emulator = lambda x: mc.run_emulation([position_model, velocity_model], x, car_initial_location)[0]\n",
    "\n",
    "def reward_function_emulator(X):\n",
    "    rewards = np.empty(shape=[0, 1])\n",
    "    for i in range(X.shape[0]):\n",
    "        rewards = np.vstack([rewards, simulator_reward_emulator(X[i,None])])\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run Bayesian optimizaton using the emulator instead of calling the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core import ParameterSpace, ContinuousParameter\n",
    "from emukit.core.initial_designs import RandomDesign\n",
    "from GPy.models import GPRegression\n",
    "from emukit.model_wrappers import GPyModelWrapper\n",
    "from emukit.bayesian_optimization.acquisitions import ExpectedImprovement\n",
    "from emukit.bayesian_optimization.loops import BayesianOptimizationLoop\n",
    "from emukit.core.loop import FixedIterationsStoppingCondition\n",
    "\n",
    "parameter_space = ParameterSpace([ContinuousParameter('postion_parameter', -1.2, 1),\n",
    "                                  ContinuousParameter('velocity_parameter', -1/0.07, 1/0.07),\n",
    "                                  ContinuousParameter('constant', -1, 1),])\n",
    "\n",
    "# Create the model\n",
    "design = RandomDesign(parameter_space) \n",
    "X = design.get_samples(25)\n",
    "Y = reward_function_emulator(X)\n",
    "kernel = GPy.kern.Matern52(3)\n",
    "model_gpy = GPRegression(X,Y,kernel = kernel) \n",
    "model_gpy.Gaussian_noise.constrain_fixed(1e-8)  ## exact evaluations of the objective\n",
    "model_gpy.optimize_restarts(5)\n",
    "model_emukit = GPyModelWrapper(model_gpy)\n",
    "model_emukit.optimize()\n",
    "\n",
    "# Create the loop\n",
    "expected_improvement = ExpectedImprovement(model = model_emukit)\n",
    "bayesopt_loop_emulator = BayesianOptimizationLoop(model = model_emukit,\n",
    "                                         space = parameter_space,\n",
    "                                         acquisition = expected_improvement,\n",
    "                                         batch_size = 1)\n",
    "\n",
    "# Run the loop\n",
    "num_evaluations = 50\n",
    "stopping_condition = FixedIterationsStoppingCondition(i_max = num_evaluations)\n",
    "bayesopt_loop_emulator.run_loop(reward_function_emulator, stopping_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as before, we use Bayesian optimization to find the best possible linear controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_opt_emulator = bayesopt_loop_emulator.loop_state.X[np.argmin(bayesopt_loop.loop_state.Y),:]\n",
    "_, _, _, frames = mc.run_simulation(env, np.atleast_2d(x_opt), render=True)\n",
    "mc.display_frames_as_gif(frames, 'Best controller after 50 iterations of Bayesian optimization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have solved the problem again! Note that in this case we have replaced the simulator of the dynamics of the car by a Gaussian process emulator that we have learned by calling the simulator only 500 times. Compared to the 37500 calls that we needed when applying Bayesian optimization directly on the simulator this is a great gain!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have analyzed the mountain car simulator from two different perspectives. First, we have optimized a linear controller by building an emulator mapping policies to reward. Second we have built an emulator of the dynamics of the system that we have used to replace the entire simulator when optimizing the control policy. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "state": {
    "339f66457bdd4f52a87e9bfcd028c0a7": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
